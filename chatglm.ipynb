{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 按顺序运行下面的block"
      ],
      "metadata": {
        "id": "38sa5hkPv1TO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0FXhdgS4H6O",
        "outputId": "7dd32913-88e6-48fd-cc08-0ad7b5b3b8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/langchain-ChatGLM-api.zip\n",
            "   creating: /content/langchain-ChatGLM-api/\n",
            "  inflating: /content/langchain-ChatGLM-api/LICENSE  \n",
            "  inflating: /content/langchain-ChatGLM-api/README.md  \n",
            "   creating: /content/langchain-ChatGLM-api/configs/\n",
            "   creating: /content/langchain-ChatGLM-api/configs/__pycache__/\n",
            "  inflating: /content/langchain-ChatGLM-api/configs/__pycache__/model_config.cpython-39.pyc  \n",
            "  inflating: /content/langchain-ChatGLM-api/configs/model_config.py  \n",
            "  inflating: /content/langchain-ChatGLM-api/cli_demo.py  \n",
            "   creating: /content/langchain-ChatGLM-api/chains/\n",
            "   creating: /content/langchain-ChatGLM-api/chains/__pycache__/\n",
            "  inflating: /content/langchain-ChatGLM-api/chains/__pycache__/local_doc_qa.cpython-39.pyc  \n",
            "  inflating: /content/langchain-ChatGLM-api/chains/local_doc_qa.py  \n",
            "   creating: /content/langchain-ChatGLM-api/docs/\n",
            "  inflating: /content/langchain-ChatGLM-api/docs/Issue-with-Installing-Packages-Using-pip-in-Anaconda.md  \n",
            "  inflating: /content/langchain-ChatGLM-api/docs/在Anaconda中使用pip安装包无效问题.md  \n",
            "  inflating: /content/langchain-ChatGLM-api/.gitignore  \n",
            "   creating: /content/langchain-ChatGLM-api/models/\n",
            "   creating: /content/langchain-ChatGLM-api/models/__pycache__/\n",
            "  inflating: /content/langchain-ChatGLM-api/models/__pycache__/chatglm_llm.cpython-39.pyc  \n",
            "  inflating: /content/langchain-ChatGLM-api/models/__pycache__/__init__.cpython-39.pyc  \n",
            "  inflating: /content/langchain-ChatGLM-api/models/chatglm_llm.py  \n",
            " extracting: /content/langchain-ChatGLM-api/models/__init__.py  \n",
            "  inflating: /content/langchain-ChatGLM-api/webui.py  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/logs/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/logs/refs/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/logs/refs/heads/\n",
            "  inflating: /content/langchain-ChatGLM-api/.git/logs/refs/heads/master  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/logs/refs/remotes/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/logs/refs/remotes/origin/\n",
            "  inflating: /content/langchain-ChatGLM-api/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/logs/HEAD  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/config  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/info/\n",
            "  inflating: /content/langchain-ChatGLM-api/.git/info/exclude  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/refs/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/refs/tags/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/refs/heads/\n",
            " extracting: /content/langchain-ChatGLM-api/.git/refs/heads/master  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/refs/remotes/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/refs/remotes/origin/\n",
            " extracting: /content/langchain-ChatGLM-api/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/packed-refs  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/index  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/objects/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/objects/info/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/objects/pack/\n",
            "  inflating: /content/langchain-ChatGLM-api/.git/objects/pack/pack-3e2879e5ade3a1c6c912d235a2c26feb4dfb61b7.pack  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/objects/pack/pack-3e2879e5ade3a1c6c912d235a2c26feb4dfb61b7.idx  \n",
            " extracting: /content/langchain-ChatGLM-api/.git/HEAD  \n",
            "   creating: /content/langchain-ChatGLM-api/.git/branches/\n",
            "   creating: /content/langchain-ChatGLM-api/.git/hooks/\n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/commit-msg.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-rebase.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/post-update.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/update.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-push.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-commit.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/hooks/pre-receive.sample  \n",
            "  inflating: /content/langchain-ChatGLM-api/.git/description  \n",
            "   creating: /content/langchain-ChatGLM-api/agent/\n",
            " extracting: /content/langchain-ChatGLM-api/agent/__init__.py  \n",
            "  inflating: /content/langchain-ChatGLM-api/api.py  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/\n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_102455/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_102455/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_102455/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101002/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101002/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101002/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101622/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101622/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101622/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103943/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103943/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103943/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101253/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101253/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_101253/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_105334/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_105334/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_105334/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104436/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104436/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104436/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103325/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103325/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_103325/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/state_of_the_search_FAISS_20230416_110319/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/state_of_the_search_FAISS_20230416_110319/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/state_of_the_search_FAISS_20230416_110319/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104915/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104915/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104915/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104104/\n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104104/index.faiss  \n",
            "  inflating: /content/langchain-ChatGLM-api/vector_store/faq_FAISS_20230416_104104/index.pkl  \n",
            "   creating: /content/langchain-ChatGLM-api/.ipynb_checkpoints/\n",
            "  inflating: /content/langchain-ChatGLM-api/requirements.txt  \n",
            "   creating: /content/langchain-ChatGLM-api/img/\n",
            "  inflating: /content/langchain-ChatGLM-api/img/langchain+chatglm.png  \n",
            "  inflating: /content/langchain-ChatGLM-api/img/qr_code.jpg  \n",
            "  inflating: /content/langchain-ChatGLM-api/img/ui1.png  \n",
            "   creating: /content/langchain-ChatGLM-api/content/\n",
            "  inflating: /content/langchain-ChatGLM-api/content/faq.txt  \n",
            "  inflating: /content/langchain-ChatGLM-api/content/langchain-ChatGLM_README.md  \n",
            "  inflating: /content/langchain-ChatGLM-api/content/state_of_the_search.txt  \n",
            "  inflating: /content/langchain-ChatGLM-api/README_en.md  \n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/imClumsyPanda/langchain-ChatGLM\n",
        "# ! zip -r langchain-ChatGLM-api.zip langchain-ChatGLM-api\n",
        "!cp /content/drive/MyDrive/langchain-ChatGLM-api.zip .\n",
        "! unzip -d /content /content/drive/MyDrive/langchain-ChatGLM-api.zip\n",
        "%cd /content/langchain-ChatGLM-api\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest-asyncio pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjheLrFRnq7F",
        "outputId": "98a20b28-8e07-42d1-a7e3-dc894dadf768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (1.5.6)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19879 sha256=cd09310639f45788bd80a129596d9da19ab0dcb6c5409f770cfabfb416df37fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/49/9c/44b13823eb256a3b4dff34b972f7a3c7d9910bfef269e59bd7\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protobuf==3.20.*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY-pYWsi6S6a",
        "outputId": "0bb116bc-45b8-47a3-839f-4d0996e72a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.3\n",
            "    Uninstalling protobuf-4.22.3:\n",
            "      Successfully uninstalled protobuf-4.22.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.11.2 which is incompatible.\n",
            "icetk 0.0.7 requires protobuf<3.19, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python api.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQqQeMeels-S",
        "outputId": "877b082a-48ed-4b54-c2fe-cc59c27e8e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/630d0efd8b49de29a5c263b5055926ec71980f50/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/630d0efd8b49de29a5c263b5055926ec71980f50/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/630d0efd8b49de29a5c263b5055926ec71980f50/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/630d0efd8b49de29a5c263b5055926ec71980f50/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "Downloading (…)58aa3/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 251kB/s]\n",
            "Downloading (…)026ff58aa3/README.md: 100% 317/317 [00:00<00:00, 115kB/s]\n",
            "Downloading (…)6ff58aa3/config.json: 100% 821/821 [00:00<00:00, 310kB/s]\n",
            "Downloading (…)aa3/eval_results.txt: 100% 69.0/69.0 [00:00<00:00, 26.3kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.30G/1.30G [00:07<00:00, 173MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 125/125 [00:00<00:00, 39.8kB/s]\n",
            "Downloading (…)58aa3/tokenizer.json: 100% 439k/439k [00:00<00:00, 685kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 514/514 [00:00<00:00, 158kB/s]\n",
            "Downloading (…)026ff58aa3/vocab.txt: 100% 110k/110k [00:00<00:00, 259kB/s]\n",
            "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
            "No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
            "state_of_the_search.txt 已成功加载\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "t=2023-04-16T11:03:20+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "Public URL: https://fbbf-34-126-130-146.ngrok.io\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m17690\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "{'prompt': '你是谁', 'history': []}\n",
            "The dtype of attention mask (torch.int64) is not bool\n",
            "2023-04-16 11:03:46.254920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "{'query': '你是谁', 'result': '你好，我是New Bing。我是一个诗人，喜欢将情感和人性的复杂性表达为艺术和文字。', 'source_documents': [Document(page_content='种子词：“你好”', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='> 他终于画完了他的画，心满意足地把它挂在了墙上。他觉得这是他一生中最伟大的作品，无人能及。他邀请了所有的朋友来欣赏，期待着他们的赞美和惊叹。 可是当他们看到画时，却没有一个人说话。他们只是互相对视，然后低头咳嗽，或者假装看手机。他感到很奇怪，难道他们都不懂艺术吗？难道他们都没有眼光吗？ 他忍不住问其中一个朋友：“你觉得我的画怎么样？” 朋友犹豫了一下，说：“嗯……其实……这个画……我以前在哪里见过。” “见过？你在哪里见过？”他惊讶地问。 “就在……就在那边啊。”朋友指了指墙角的一个小框架，“那不就是你上个月买回来的那幅名画吗？你怎么把它照抄了一遍？                                                             ——New Bing', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'UncategorizedText'}), Document(page_content='提示公式：“请根据以下种子词生成文本：你好”', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='命名实体识别（NER）是一种技术，它可以使模型识别和分类文本中的命名实体，例如人名、组织机构、地点和日期等。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='指令：故事应基于一组给定的角色和特定的主题', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='问答提示是一种技术，可以让模型生成回答特定问题或任务的文本。通过将问题或任务与可能与问题或任务相关的任何其他信息一起作为输入提供给模型来实现此目的。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='情感分析是一种技术，允许模型确定文本的情绪色彩或态度，例如它是积极的、消极的还是中立的。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='1. 确定您要讨论的主题或想法。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='这是一种利用模型预先存在的知识来生成新的信息或回答问题的技术。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='角色：诗人', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'})]}\n",
            "[2023-04-16 11:03:48] \", prompt:\"你是谁\", response:\"{'query': '你是谁', 'result': '你好，我是New Bing。我是一个诗人，喜欢将情感和人性的复杂性表达为艺术和文字。', 'source_documents': [Document(page_content='种子词：“你好”', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='> 他终于画完了他的画，心满意足地把它挂在了墙上。他觉得这是他一生中最伟大的作品，无人能及。他邀请了所有的朋友来欣赏，期待着他们的赞美和惊叹。 可是当他们看到画时，却没有一个人说话。他们只是互相对视，然后低头咳嗽，或者假装看手机。他感到很奇怪，难道他们都不懂艺术吗？难道他们都没有眼光吗？ 他忍不住问其中一个朋友：“你觉得我的画怎么样？” 朋友犹豫了一下，说：“嗯……其实……这个画……我以前在哪里见过。” “见过？你在哪里见过？”他惊讶地问。 “就在……就在那边啊。”朋友指了指墙角的一个小框架，“那不就是你上个月买回来的那幅名画吗？你怎么把它照抄了一遍？                                                             ——New Bing', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'UncategorizedText'}), Document(page_content='提示公式：“请根据以下种子词生成文本：你好”', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='命名实体识别（NER）是一种技术，它可以使模型识别和分类文本中的命名实体，例如人名、组织机构、地点和日期等。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='指令：故事应基于一组给定的角色和特定的主题', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='问答提示是一种技术，可以让模型生成回答特定问题或任务的文本。通过将问题或任务与可能与问题或任务相关的任何其他信息一起作为输入提供给模型来实现此目的。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='情感分析是一种技术，允许模型确定文本的情绪色彩或态度，例如它是积极的、消极的还是中立的。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='1. 确定您要讨论的主题或想法。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='这是一种利用模型预先存在的知识来生成新的信息或回答问题的技术。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='角色：诗人', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'})]}\"\n",
            "\u001b[32mINFO\u001b[0m:     1.36.166.176:0 - \"\u001b[1mPOST / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "{'prompt': 'ChatGPT是什么', 'history': []}\n",
            "{'query': 'ChatGPT是什么', 'result': 'ChatGPT是一种基于自然语言处理技术的人工智能模型，由OpenAI开发。它可以进行情感分析、文本分类、机器翻译等多种自然语言处理任务。通过与模型进行交互，可以要求其对某个文本进行情感分类，或者对其内容进行续写等。ChatGPT的工作原理是将输入的文本转化为向量，然后将其输入到模型中，由模型对其进行情感分类或文本生成。', 'source_documents': [Document(page_content='*如何在ChatGPT中使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='*如何在ChatGPT中使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='*如何与ChatGPT一起使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='要在ChatGPT中使用情感分析提示，模型应该提供一段文本并要求根据其情感分类。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='Prompt 工程是创建提示或指导像 ChatGPT 这样的语言模型输出的过程。它允许用户控制模型的输出并生成符合其特定需求的文本。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='在本书中，我们将探讨可用于 ChatGPT 的各种 Prompt 工程技术。我们将讨论不同类型的提示，以及如何使用它们实现您想要的特定目标。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='什么是 Prompt 工程？', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='然而，为了从 ChatGPT 中获得最佳结果，重要的是要了解如何正确地提示模型。 提示可以让用户控制模型的输出并生成相关、准确和高质量的文本。 在使用 ChatGPT 时，了解它的能力和限制非常重要。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='# 如何向 ChatGPT 提问以获得高质量答案：提示技巧工程完全指南', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='这个提示要求对特定主题或想法展开对话或讨论。发言者邀请ChatGPT参与讨论相关主题。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'})]}\n",
            "[2023-04-16 11:05:04] \", prompt:\"ChatGPT是什么\", response:\"{'query': 'ChatGPT是什么', 'result': 'ChatGPT是一种基于自然语言处理技术的人工智能模型，由OpenAI开发。它可以进行情感分析、文本分类、机器翻译等多种自然语言处理任务。通过与模型进行交互，可以要求其对某个文本进行情感分类，或者对其内容进行续写等。ChatGPT的工作原理是将输入的文本转化为向量，然后将其输入到模型中，由模型对其进行情感分类或文本生成。', 'source_documents': [Document(page_content='*如何在ChatGPT中使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='*如何在ChatGPT中使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='*如何与ChatGPT一起使用：**', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'ListItem'}), Document(page_content='要在ChatGPT中使用情感分析提示，模型应该提供一段文本并要求根据其情感分类。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='Prompt 工程是创建提示或指导像 ChatGPT 这样的语言模型输出的过程。它允许用户控制模型的输出并生成符合其特定需求的文本。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='在本书中，我们将探讨可用于 ChatGPT 的各种 Prompt 工程技术。我们将讨论不同类型的提示，以及如何使用它们实现您想要的特定目标。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='什么是 Prompt 工程？', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='然而，为了从 ChatGPT 中获得最佳结果，重要的是要了解如何正确地提示模型。 提示可以让用户控制模型的输出并生成相关、准确和高质量的文本。 在使用 ChatGPT 时，了解它的能力和限制非常重要。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='# 如何向 ChatGPT 提问以获得高质量答案：提示技巧工程完全指南', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'}), Document(page_content='这个提示要求对特定主题或想法展开对话或讨论。发言者邀请ChatGPT参与讨论相关主题。', metadata={'source': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'filename': '/content/langchain-ChatGLM-api/content/state_of_the_search.txt', 'category': 'Title'})]}\"\n",
            "\u001b[32mINFO\u001b[0m:     1.36.166.176:0 - \"\u001b[1mPOST / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m17690\u001b[0m]\n"
          ]
        }
      ]
    }
  ]
}